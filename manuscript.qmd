---
title: "Meta-analysis in Plant Pathology"
format: 
  html:
    toc: true
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r}
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(gsheet)
library(janitor)
library(ggthemes)
library(cowplot)
library(colorspace)
library(patchwork)
library(wordcloud)
```

```{r}
dat <- gsheet2tbl("https://docs.google.com/spreadsheets/d/1vYXB1Ag-ouLgo9nLIelP1V0hz-ki0f7p-aOCAkmuxKI/edit#gid=1058316481")
theme_set(theme_minimal_grid())
```

# Bibliographic info

## Total number of pubs

```{r}
nrow(dat)

dat |> 
  tabyl(article_type)

```

## Pubs per year

```{r}

dat |> 
  filter(pub_year > 2010) |> 
  nrow()

```

## Pub type by year

```{r}
p1 <- dat %>% 
  tabyl(pub_year, article_type) %>% 
  pivot_longer(names_to = "Type", 
               values_to = "n", 2:3) %>% 
  ggplot(aes(pub_year, n, fill = Type))+
 geom_col()+
  scale_x_continuous(breaks = c(1999, 2001, 2004, 2007, 2010, 2013, 2016,
                                2019, 2022))+
  theme(legend.position = "bottom",
          panel.grid.major=element_line(colour="grey94"))+
 scale_fill_discrete_qualitative(palette = "cold")+
  scale_y_continuous(n.breaks = 10)+
  labs( x = "Publication year", y = "Number of publications")


```

## Journals

```{r}
tab2 <- dat %>% 
  dplyr::select(journal) %>% 
   tabyl(journal) %>% 
  select(-percent) |> 
  arrange(-n)

tab2

nrow(tab2)

```

```{r}
#| fig-width: 14
#| fig-height: 7
set.seed(3)
old_par <- par(mar = c(0, 2, 0, 0), bg = NA)
p1 + wrap_elements(panel = ~wordcloud(words = tab2$journal, freq = tab2$n,  min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.25, colors=brewer.pal(6, "Dark2"))) + plot_annotation(tag_levels = "A")
par(old_par) 
ggsave("figs/figure1.png", width = 15, height = 8, bg = "white")


```

## Number of authors per publication

```{r}
dat %>% 
  tabyl(n_authors) 

dat |> 
  tabyl(n_authors) |> 
  summary()

dat |> 
  tabyl(n_authors) |> 
  ggplot(aes(n_authors, n))+
  geom_col(fill = "#ACA4E2")+
  scale_y_continuous(n.breaks = 10)+
  scale_x_continuous(n.breaks = 10)+
  theme(legend.position = "bottom",
          panel.grid.major=element_line(colour="grey94"))+
  labs(y = "Frequency", x = "Number of authors per paper")
ggsave("figs/authors_paper.png", width = 5, height = 4, bg = "white")


```

## Unique authors

```{r}

dat_authors <- gsheet2tbl("https://docs.google.com/spreadsheets/d/1vYXB1Ag-ouLgo9nLIelP1V0hz-ki0f7p-aOCAkmuxKI/edit#gid=1102752782")
authors <- dat_authors %>% 
  gather(author, name, 2:32) %>% 
  select(author, name) %>% 
  filter(name != "NA") %>% 
  group_by(name) %>% 
  tally(sort = T) 

# Unique authors
nrow(authors)

#paper per author
authors

write_csv(authors, "authors.csv")

summary(authors$n)

authors |> 
  filter(n == 1)

```

## Authorship network

```{r}
library(purrr)
library(purrrlyr)

authors_net <- dat_authors %>% select (2:32)
author_list <- flatten(by_row(authors_net, ..f = function(x) flatten_chr(x), .labels = FALSE))
author_list <- lapply(author_list, function(x) x[!is.na(x)])

# create the edge list
author_edge_list <- t(do.call(cbind, lapply(author_list[sapply(author_list, length) >= 2], combn, 2)))

author_edge_list[1:10, ]





```

Within an authorship network, co-authors (present in a same article) are linked together. Authors from this articles can be connected to authors from other articles whenever they appear together. Therefore, two articles are linked by a common author. Each author is then considered a **node** in the network and the connections between them are the **edges** or links. There are several statistics to calculate in a network analysis.

For now, let's visualize the authorship network and also the community structure which was defined via a function that tries to find densely connected subgraphs, also called communities. We will use a random walk algorithm for determining the communities. The idea is that short random walks tend to stay in the same community. In the network below, the subgraphs are represented by the colors.

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
# igraph
library(igraph)
net=graph.edgelist(as.matrix(author_edge_list), directed=FALSE)

# https://www.r-econometrics.com/networks/network-summary/

#The degree of a node is the number of its connections. The degree function calculates this number for each node of a graph. The node with the highest number is the node with the highest number of connections.

hist(degree(net))
hist(log(degree(net)))
degree <- enframe(degree(net))
degree %>%  arrange(-value) |> head(10)

#Closeness centrality describes how close a given node is to any other node. It is usually defined as the inverse of the average of the shortest path between a node and all other nodes. Therefore, shorter paths between a node and any other node in the graph imply a higher value of the ratio. In constrast to the degree of a node, which describes the number of its direct connections, its closeness provides an idea of how well a node is indirectly connected via other nodes.

close <-data.frame(round(closeness(net), 10))
close |> arrange(-round.closeness.net...10.)|> head(10)

# Freeman (1977) proposes betweenness centrality as the number of shortest paths passing through a node. A higher value of a node impilies that other nodes are well connected through it.
between <- data.frame(round(betweenness(net), 1))
between |> arrange(-round.betweenness.net...1.)|> head(10)

page <- data.frame(page_rank(net)$vector)
page |> arrange(-page_rank.net..vector)|> head(10)

# Eigenvector centrality (Bonabeau, 1972) is based on the idea that the importance of a node is recusively related to the importance of the nodes pointing to it. For example, your popularity depends on the popularity of your friends, whose popularity depends on their friends etc. Therefore, this measure is also self-referential in the sense that a node’s centrality depends on the centrality of another node, whose centrality depends the first node. A higher value of eigenvector centrality implies that a node’s neighbours are more prestigious than the neighbours of other nodes.
eigen <- data.frame(round(evcent(net)$vector, 5))
eigen |> arrange(-round.evcent.net..vector..5.)|> head(10)

# Authority score is another measure of centrality initially applied to the Web. A node has high authority when it is linked by many other nodes that are linking many other nodes. 
authority <- data.frame(authority_score(net)$vector)
authority |> arrange(-authority_score.net..vector)|> head(10)

# Collect the different centrality measures in a data frame
df <- data.frame(degree(net),
                          closeness(net),
                           betweenness(net),
                           eigen_centrality(net)$vector)

# Scatterplot matrix
pairs(df)

#Network properties: Let’s now try to describe what a network looks like as a whole. We can start with measures of the size of a network. diameter is the length of the longest path (in number of edges) between two nodes. We can use get_diameter to identify this path. mean_distance is the average number of edges between any two nodes in the network. We can find each of these paths between pairs of edges with distances. 

diameter(net, directed = FALSE, weights = NA)

get_diameter(net)

mean_distance(net)

# edge_density is the proportion of edges in the network over all possible edges that could exist.

edge_density(net)

# reciprocity measures the propensity of each edge to be a mutual edge; that is, the probability that if i is connected to j, j is also connected to i.

reciprocity(net)

#transitivity, also known as clustering coefficient, measures that probability that adjacent nodes of a network are connected. In other words, if i is connected to j, and j is connected to k, what is the probability that i is also connected to k?

transitivity(net)

# Network communities - Networks often have different clusters or communities of nodes that are more densely connected to each other than to the rest of the network. Let’s cover some of the different existing methods to identify these communities. The most straightforward way to partition a network is into connected components. Each component is a group of nodes that are connected to each other, but not to the rest of the nodes. For example, this network has two components.


```

### Network graph

```{r cache=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(network)
library(intergraph)

# Clusters


# The walktrap algorithm finds communities through a series of short random walks. The idea is that these random walks tend to stay within the same community. The length of these random walks is 4 edges by default, but you may want to experiment with different values. The goal of this algorithm is to identify the partition that maximizes a modularity score.

wc <- cluster_walktrap(net)

#The edge-betweenness method iteratively removes edges with high betweenness, with the idea that they are likely to connect different parts of the network. Here betweenness (gatekeeping potential) applies to edges, but the intuition is the same.
eb <- cluster_edge_betweenness(net)

lec <- cluster_leading_eigen(net)

#T he label propagation method labels each node with unique labels, and then updates these labels by choosing the label assigned to the majority of their neighbors, and repeat this iteratively until each node has the most common labels among its neighbors. 
cl <- cluster_label_prop(net)

# Modularity
mod <- modularity(wc)
ms <- membership(wc)

net_stat <- asNetwork(net)
png("figs/network1.png", res = 600,  width = 5000 , height = 5000, units="px")
set.seed(11)
par(mar=c(0,0,0,0))
plot.network(net_stat, vertex.cex= 0.05 + 0.25*log(graph.strength(net)), 
             label =ifelse(degree(net)>1000,V(net)$name,NA), label.bg = "white", label.col = "black", edge.col = "lightgray", edge.lty = 0.5, label.cex = 0.6,  displaylabels = FALSE, vertex.col = membership(eb), jitter = T, edge.len = 0.2, boxed.labels=T, label.border=1, pad=5)
dev.off()
```

![](figs/network1.png){fig-align="center"}

```{r}
#| warning: false
library(networkD3)


wc <- cluster_walktrap(net)
members <- membership(wc)
net2 <- igraph_to_networkD3(net, group = members)
forceNetwork(Links = net2$links, Nodes = net2$nodes, 
             Source = 'source', Target = 'target', 
             NodeID = 'name', Group = 'group') |> 
  saveNetwork(file = 'figs/net.html')



```

```{r}
# create a csv file of the network
write_csv(as_long_data_frame(net), file = "rede.csv")



```

# Data characteristics

## Source

```{r}
p2 <- dat %>% 
  filter(article_type == "Original Article") %>% 
  tabyl(data_source) %>% 
  ggplot(aes(reorder(data_source, -n), n, fill = n))+
  geom_col(fill = "#ACA4E2", width = 0.56)+
  geom_text(
    aes(x = data_source, y = n, label = n),
    position = position_dodge(width = 1),
    vjust = -0.5, size = 4) + 
  theme(legend.position = "bottom",
          panel.grid.major=element_line(colour="grey94"))+
scale_y_continuous(breaks = c(0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30))+
  labs(x = "Source of the data used in the analysis", y = "Number of original articles")
 p2
 ggsave("figs/figure2.png", width =8, height = 6, bg = "white")

```

## Systematic review in PR?

```{r}

dat |> 
  tabyl(systematic_review, data_source)

```

## PRISM diagram?

```{r}

dat |> 
  tabyl(sr_flow_diag)

```

## Shared?

```{r}
dat |> 
  tabyl(data_shared)
```

# Study characteristics

## Number of trials

```{r}
#| warning: false
dat |> 
  count(n_trials_total) |> 
  ggplot(aes(n_trials_total))+
  geom_histogram(color = "white", fill = "#ACA4E2")+
  labs(x = "N. of trials/studies", y = "Frequency")+
  scale_x_continuous(n.breaks = 10)+
  theme(legend.position = "bottom",
          panel.grid.major=element_line(colour="grey94"))
ggsave("figs/trials_study.png", width =6, height =4, bg = "white")

dat |> 
  count(n_trials_total) |> 
  summary()
```

## By objective and product type

```{r}
objective <- dat %>% 
  filter(article_type == "Original Article") %>% 
  tabyl(objective) |> 
  select(-percent)

type <- dat %>% 
  filter(article_type == "Original Article") %>% 
  filter(objective == "Product effects") %>% 
  tabyl(product_type) |> 
  select(-percent)
cbind(objective, type)

```

## Response variables

```{r}
tab <- dat %>% 
  dplyr::select(response1 , response2, response3, response4, response5) %>% 
  pivot_longer(names_to = "type", values_to = "Variable", 1:5) %>% 
  select(Variable) %>%
    filter(Variable != "NA") %>% 
  tabyl(Variable) %>% 
  select(-percent)
nrow(tab)
tab

library(wordcloud)
wordcloud(words = tab$Variable, freq = tab$n,  min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.25,            colors=brewer.pal(5, "Dark2"))


```

## Number of responses per study

```{r}
dat |> 
  tabyl(n_responses)
```

# Meta-analysis model characteristics

## Effect sizes

```{r}
es <- dat %>% 
  dplyr::select(effect_size_1, effect_size_2, effect_size_3, effect_size_4, effect_size_5) %>% 
  pivot_longer(names_to = "type", values_to = "value", 1:5) %>% 
  select(value) %>%
    filter(value != "NA") %>% 
  tabyl(value) |> 
  adorn_totals()
es
write_csv(es, "es.csv")
```

## Effect-size by common response variable

```{r}
es <- dat %>% 
  dplyr::select(code, effect_size_1, effect_size_2, effect_size_3, effect_size_4, effect_size_5) %>% 
  pivot_longer(names_to = "type", values_to = "value", 2:6)

rv <- dat %>% 
  dplyr::select(code, response1 , response2, response3, response4, response5) %>% 
  pivot_longer(names_to = "type", values_to = "Variable", 2:6)

rv

rv2 <- left_join(es, rv, by = "code") |> 
  select(Variable, value) |> 
  filter(Variable %in% c("severity", "incidence",  "yield", "index", "slope", "intercept")) |> 
  tabyl(value, Variable)

rv2 

write_csv(rv2, "es2.csv")
```

## Sampling variance

```{r}
dat |> 
  tabyl(sampling_var)

```

## Heterogeneity test

```{r}
dat |> 
  tabyl(`Heterogenity test`)
```

## General approach

```{r}
 
dat |> 
  tabyl(ma_approach)
```

## MA basic model

```{r}
dat |> 
  tabyl(ma_model)
```

## MA model n. of effects

```{r}
dat |> 
  tabyl(ma_model_2)
```

## Number of variables

```{r}
dat |> 
  tabyl(ma_n_variables)
```

## Moderator analysis?

```{r}
dat |> 
  tabyl(moderator)

dat |> 
  tabyl(moderator_model)
```

# Software characteristics

## General software

```{r}
software <- dat |> 
  tabyl(general_software) |> 
  arrange(general_software)

software

write_csv(software, "software.csv")


```

```{r}
dat |> 
  tabyl(MA_software)
```

# Data summary

## Results in table?

```{r}
dat |> 
  tabyl(res_table)
```

## Results in plot for raw data

```{r}
dat |> 
  tabyl(res_plot_raw)
```

## Result in forest plot

```{r}
dat |> 
  tabyl(res_forest)
```

# Economic analysis

```{r}
dat |> 
  tabyl(econ_analysis)
```
